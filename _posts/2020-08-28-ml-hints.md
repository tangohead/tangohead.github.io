---
layout: post
title: Machine Learning Hints
#description: 
#summary: 
comments: false
tags: [machine learning]
---

Often I come across posts or links to great explainers on ML problems, quirks or gotchas. I figured I should keep them in one place, so I'll update this for now but might make a dedicated page at some point.

## "Classic" ML

### Trees

[Does one-hot encoding make tree/random forest performance worse?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)

### Regularisation

[Why does L1 encourage sparse models?](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models)

[Why do we use L1 and L2 norm but not others?](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms)


## Neural Nets

### Regularisation 

[Why does Batch Normalisation have a regularising effect?](https://www.quora.com/Is-there-a-theory-for-why-batch-normalization-has-a-regularizing-effect)